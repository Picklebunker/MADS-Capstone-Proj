{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3768c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95afc1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: surprise in ./.local/lib/python3.9/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in ./.local/lib/python3.9/site-packages (from surprise) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.20.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "\n",
    "from surprise import Dataset, Reader, BaselineOnly, KNNBasic, accuracy\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a59e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c831807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1149780, 3) (278858, 3) (271360, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "ratings_data=pd.read_csv('Ratings.csv')\n",
    "users_data=pd.read_csv('Users.csv')\n",
    "books_data=pd.read_csv('Books.csv')\n",
    "\n",
    "print(ratings_data.shape, users_data.shape, books_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8b7c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records with null publication years:  3\n",
      "1 (271357, 5)\n",
      "Count:  13 \n",
      "               ISBN                                         Book-Title  \\\n",
      "37487   0671746103  MY TEACHER FRIED MY BRAINS (RACK SIZE) (MY TEA...   \n",
      "55676   0671791990  MY TEACHER FLUNKED THE PLANET (RACK SIZE) (MY ...   \n",
      "78168   0870449842                                   Crossing America   \n",
      "80264   0140301690  Alice's Adventures in Wonderland and Through t...   \n",
      "97826   0140201092      Outline of European Architecture (Pelican S.)   \n",
      "116053  0394701658                       Three Plays of Eugene Oneill   \n",
      "118294  3442436893        Das groÃ?Â?e BÃ?Â¶se- MÃ?Â¤dchen- Lesebuch.   \n",
      "192993  0870446924  Field Guide to the Birds of North America, 3rd...   \n",
      "228173  0671266500       FOREST PEOPLE (Touchstone Books (Hardcover))   \n",
      "240169  0684718022            In Our Time: Stories (Scribner Classic)   \n",
      "246842  0380000059                                              CLOUT   \n",
      "255409  068471809X                               To Have and Have Not   \n",
      "260974  0671740989        FOOTBALL SUPER TEAMS : FOOTBALL SUPER TEAMS   \n",
      "\n",
      "                        Book-Author  Year-Of-Publication  \\\n",
      "37487                       Coville               2030.0   \n",
      "55676                 Bruce Coville               2030.0   \n",
      "78168   National Geographic Society               2030.0   \n",
      "80264                 Lewis Carroll               2050.0   \n",
      "97826              Nikolaus Pevsner               2050.0   \n",
      "116053               Eugene O'Neill               2038.0   \n",
      "118294                  Kathy Lette               2026.0   \n",
      "192993  National Geographic Society               2030.0   \n",
      "228173            Colin M. Turnbull               2030.0   \n",
      "240169             Ernest Hemingway               2030.0   \n",
      "246842                   D. GIBBONS               2024.0   \n",
      "255409             Ernest Hemingway               2037.0   \n",
      "260974                  Bill Gutman               2030.0   \n",
      "\n",
      "                                         Publisher  \n",
      "37487                                      Aladdin  \n",
      "55676                                      Aladdin  \n",
      "78168                          National Geographic  \n",
      "80264                                 Puffin Books  \n",
      "97826                                  Penguin USA  \n",
      "116053                           Vintage Books USA  \n",
      "118294                                    Goldmann  \n",
      "192993                         National Geographic  \n",
      "228173                        Simon &amp; Schuster  \n",
      "240169                               Collier Books  \n",
      "246842                                        Avon  \n",
      "255409                        Simon &amp; Schuster  \n",
      "260974  Simon &amp; Schuster Children's Publishing  \n",
      "2 (271341, 5)\n",
      "Empty DataFrame\n",
      "Columns: [ISBN, Book-Title, Book-Author, Year-Of-Publication, Publisher]\n",
      "Index: []\n",
      "3 (271341, 5)\n",
      "4 (278858, 3)\n",
      "5 (168096, 3)\n"
     ]
    }
   ],
   "source": [
    "#pre-processing\n",
    "\n",
    "'''\n",
    "On handling nulls\n",
    "For this dataset, since there are not many features while there are only 4 records with nulls,\n",
    "I think it would be best to just drop the NA rows.\n",
    "'''\n",
    "#Remove image url columns\n",
    "books=books_data.iloc[:,:5]\n",
    "books=books.dropna()\n",
    "\n",
    "\n",
    "#Publication year\n",
    "\n",
    "#Convert publication year to int\n",
    "books['Year-Of-Publication']=pd.to_numeric(books['Year-Of-Publication'],errors='coerce')\n",
    "\n",
    "#Publication years that are null\n",
    "print('Count of records with null publication years: ', len(books[books['Year-Of-Publication'].isna()]), \n",
    "     )\n",
    "books.dropna(subset=['Year-Of-Publication'])\n",
    "print(1, books.shape)\n",
    "\n",
    "\n",
    "#and Publication years years that are in the future. These books and their reviews shouldn't be included\n",
    "print('Count: ', len(books[books['Year-Of-Publication']>2023]),\n",
    "      '\\n',books[books['Year-Of-Publication']>2023]\n",
    "     )\n",
    "books=books[books['Year-Of-Publication']<2023]\n",
    "print(2, books.shape)\n",
    "\n",
    "#Book author\n",
    "#There are 3 records where the value for author are PRESUMABLY publication years. The same 3 that had null publication years. We'll get rid of them.   \n",
    "print(books[books['Book-Author'].str.contains('^\\d{4}$')])\n",
    "books=books[~books['Book-Author'].str.contains('^\\d{4}$')]\n",
    "print(3, books.shape)\n",
    "\n",
    "#filter by year of publication\n",
    "books=books[books['Year-Of-Publication']>1900]\n",
    "\n",
    "#ISBN remove non alpha-numeric characters from ISBN\n",
    "books['ISBN'] = books['ISBN'].replace('[^a-zA-Z0-9]', '', regex=True)\n",
    "ratings_data['ISBN'] = ratings_data['ISBN'].replace('[^a-zA-Z0-9]', '', regex=True)\n",
    "\n",
    "\n",
    "#Remove outlier age\n",
    "\n",
    "users_data[(users_data['Age']>0)&(users_data['Age']<100)]\n",
    "print(4, users_data.shape)\n",
    "\n",
    "#Dropping null age since we dont have that many features to beging with \n",
    "#and having this much nulls in one of the features is bad\n",
    "users_data=users_data.dropna()\n",
    "print(5, users_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00ae7d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3975913/1080461751.py:18: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  users['Country'] = users['Country'].str.replace('[^a-zA-Z\\.\\ ]', '')\n",
      "/tmp/ipykernel_3975913/1080461751.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  users['State'] = users['State'].str.replace('[^a-zA-Z\\.\\ ]', '')\n",
      "/tmp/ipykernel_3975913/1080461751.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  users['City'] = users['City'].str.replace('[^a-zA-Z\\.\\ ]', '')\n"
     ]
    }
   ],
   "source": [
    "#Duke's preprocessing\n",
    "\n",
    "users=users_data.copy()\n",
    "\n",
    "# add a new column, age group, to label users' age\n",
    "\n",
    "bins= [0,2,4,13,20,110]\n",
    "labels = ['Infant','Toddler','Kid','Teen','Adult']\n",
    "users['AgeGroup'] = pd.cut(users['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# add a columns: Country,State,and City, to label users' country, state, and city.\n",
    "\n",
    "users['Country'] = users['Location'].str.split(',').str[-1]\n",
    "users['State'] = users['Location'].str.split(',').str[-2]\n",
    "users['City'] = users['Location'].str.split(',').str[-3]\n",
    "\n",
    "# Process the country column to only retain alphabetic letters+space+period as no country name contain numbers, special characters\n",
    "users['Country'] = users['Country'].str.replace('[^a-zA-Z\\.\\ ]', '')\n",
    "users['State'] = users['State'].str.replace('[^a-zA-Z\\.\\ ]', '')\n",
    "users['City'] = users['City'].str.replace('[^a-zA-Z\\.\\ ]', '')\n",
    "\n",
    "# strip off the leading and trailing white spaces\n",
    "users['Country'] = users['Country'] .str.strip()\n",
    "users['State'] = users['State'] .str.strip()\n",
    "users['City'] = users['City'] .str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5eb8a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76495, 7)\n",
      "(1013453, 3)\n"
     ]
    }
   ],
   "source": [
    "#selecting only usa data in attpemt to refine data\n",
    "users=users[users.Country=='usa']\n",
    "print(users.shape)\n",
    "\n",
    "#remove redundancy\n",
    "users=users.loc[:,['User-ID','AgeGroup','State','City']]\n",
    "\n",
    "#selecting only users with more than 5 book ratings\n",
    "ratings=ratings_data.copy()\n",
    "ratings=ratings.groupby('User-ID').filter(lambda x: len(x) > 5)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02403675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>AgeGroup</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>2954</td>\n",
       "      <td>8</td>\n",
       "      <td>Adult</td>\n",
       "      <td>kansas</td>\n",
       "      <td>wichita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0440235502</td>\n",
       "      <td>October Sky: A Memoir</td>\n",
       "      <td>Homer Hickam</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Dell</td>\n",
       "      <td>2954</td>\n",
       "      <td>10</td>\n",
       "      <td>Adult</td>\n",
       "      <td>kansas</td>\n",
       "      <td>wichita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0380973499</td>\n",
       "      <td>War's End: An Eyewitness Account of America's ...</td>\n",
       "      <td>Charles W. Sweeney</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>William Morrow &amp;amp; Company</td>\n",
       "      <td>2954</td>\n",
       "      <td>7</td>\n",
       "      <td>Adult</td>\n",
       "      <td>kansas</td>\n",
       "      <td>wichita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0684867184</td>\n",
       "      <td>Comrades : Brothers, Fathers, Heroes, Sons, Pals</td>\n",
       "      <td>Stephen E. Ambrose</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Simon &amp;amp; Schuster</td>\n",
       "      <td>2954</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult</td>\n",
       "      <td>kansas</td>\n",
       "      <td>wichita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>031224116X</td>\n",
       "      <td>Murder on the Mauretania (George Porter Dillma...</td>\n",
       "      <td>Conrad Allen</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>St. Martin's Minotaur</td>\n",
       "      <td>2954</td>\n",
       "      <td>8</td>\n",
       "      <td>Adult</td>\n",
       "      <td>kansas</td>\n",
       "      <td>wichita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510838</th>\n",
       "      <td>0761101861</td>\n",
       "      <td>Beauty, The New Basics</td>\n",
       "      <td>Rona Berg</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>251903</td>\n",
       "      <td>10</td>\n",
       "      <td>Adult</td>\n",
       "      <td>washington</td>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510839</th>\n",
       "      <td>0882669850</td>\n",
       "      <td>Perfumes, Splashes &amp;amp; Colognes: Discovering...</td>\n",
       "      <td>Nancy M. Booth</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Storey Publishing</td>\n",
       "      <td>251903</td>\n",
       "      <td>10</td>\n",
       "      <td>Adult</td>\n",
       "      <td>washington</td>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510840</th>\n",
       "      <td>0877739870</td>\n",
       "      <td>Personality Type (Jung on the Hudson Book Series)</td>\n",
       "      <td>Lenore Thomson</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Shambhala</td>\n",
       "      <td>251903</td>\n",
       "      <td>8</td>\n",
       "      <td>Adult</td>\n",
       "      <td>washington</td>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510841</th>\n",
       "      <td>0825431654</td>\n",
       "      <td>The Doctrines That Divide: A Fresh Look at the...</td>\n",
       "      <td>Erwin W. Lutzer</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Kregel Publications</td>\n",
       "      <td>251903</td>\n",
       "      <td>10</td>\n",
       "      <td>Adult</td>\n",
       "      <td>washington</td>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510842</th>\n",
       "      <td>0415158044</td>\n",
       "      <td>A History of Pagan Europe</td>\n",
       "      <td>Prudence Jones</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Routledge</td>\n",
       "      <td>251903</td>\n",
       "      <td>0</td>\n",
       "      <td>Adult</td>\n",
       "      <td>washington</td>\n",
       "      <td>seattle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510843 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISBN                                         Book-Title  \\\n",
       "0       0060973129                               Decision in Normandy   \n",
       "1       0440235502                              October Sky: A Memoir   \n",
       "2       0380973499  War's End: An Eyewitness Account of America's ...   \n",
       "3       0684867184   Comrades : Brothers, Fathers, Heroes, Sons, Pals   \n",
       "4       031224116X  Murder on the Mauretania (George Porter Dillma...   \n",
       "...            ...                                                ...   \n",
       "510838  0761101861                             Beauty, The New Basics   \n",
       "510839  0882669850  Perfumes, Splashes &amp; Colognes: Discovering...   \n",
       "510840  0877739870  Personality Type (Jung on the Hudson Book Series)   \n",
       "510841  0825431654  The Doctrines That Divide: A Fresh Look at the...   \n",
       "510842  0415158044                          A History of Pagan Europe   \n",
       "\n",
       "               Book-Author  Year-Of-Publication                     Publisher  \\\n",
       "0             Carlo D'Este               1991.0               HarperPerennial   \n",
       "1             Homer Hickam               1999.0                          Dell   \n",
       "2       Charles W. Sweeney               1997.0  William Morrow &amp; Company   \n",
       "3       Stephen E. Ambrose               1999.0          Simon &amp; Schuster   \n",
       "4             Conrad Allen               2000.0         St. Martin's Minotaur   \n",
       "...                    ...                  ...                           ...   \n",
       "510838           Rona Berg               2001.0            Workman Publishing   \n",
       "510839      Nancy M. Booth               1997.0             Storey Publishing   \n",
       "510840      Lenore Thomson               1998.0                     Shambhala   \n",
       "510841     Erwin W. Lutzer               1998.0           Kregel Publications   \n",
       "510842      Prudence Jones               1997.0                     Routledge   \n",
       "\n",
       "        User-ID  Book-Rating AgeGroup       State     City  \n",
       "0          2954            8    Adult      kansas  wichita  \n",
       "1          2954           10    Adult      kansas  wichita  \n",
       "2          2954            7    Adult      kansas  wichita  \n",
       "3          2954            0    Adult      kansas  wichita  \n",
       "4          2954            8    Adult      kansas  wichita  \n",
       "...         ...          ...      ...         ...      ...  \n",
       "510838   251903           10    Adult  washington  seattle  \n",
       "510839   251903           10    Adult  washington  seattle  \n",
       "510840   251903            8    Adult  washington  seattle  \n",
       "510841   251903           10    Adult  washington  seattle  \n",
       "510842   251903            0    Adult  washington  seattle  \n",
       "\n",
       "[510843 rows x 10 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joining data\n",
    "joined_data=pd.merge(books, ratings, on='ISBN', how='inner')\n",
    "joined_data=pd.merge(joined_data, users, on='User-ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd56a153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=joined_data\n",
    "df['User-ID'] = df['User-ID'].astype(int)\n",
    "df['AgeGroup'] = df['AgeGroup'].astype(str)\n",
    "df['Book-Rating'] = df['Book-Rating'].astype(int)\n",
    "# df['Year-Of-Publication'] = df['Year-Of-Publication'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "df['Year-Of-Publication'] = df['Year-Of-Publication'].astype(int)\n",
    "df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06bd5865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 510843 entries, 0 to 510842\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   ISBN                 510843 non-null  object\n",
      " 1   Book-Title           510843 non-null  object\n",
      " 2   Book-Author          510843 non-null  object\n",
      " 3   Year-Of-Publication  510843 non-null  int64 \n",
      " 4   Publisher            510843 non-null  object\n",
      " 5   User-ID              510843 non-null  int64 \n",
      " 6   Book-Rating          510843 non-null  int64 \n",
      " 7   AgeGroup             510843 non-null  object\n",
      " 8   State                510843 non-null  object\n",
      " 9   City                 510843 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 42.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30346627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a custom transformer for feature engineering\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Create a feature representing the age of a book\n",
    "#         X['AgeOfBook'] = pd.Timestamp.now().year - X['Year-Of-Publication'].astype(int)\n",
    "        return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3423990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset \n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = df.copy()\n",
    "\n",
    "\n",
    "# To address leakage\n",
    "unique_user_ids = data['User-ID'].unique()\n",
    "train_user_ids, test_user_ids = train_test_split(unique_user_ids, test_size=0.2, random_state=42)\n",
    "trainset = data[data['User-ID'].isin(train_user_ids)]\n",
    "testset = data[data['User-ID'].isin(test_user_ids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69c43000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 3.6987\n",
      "MAE:  3.3494\n",
      "RMSE on test set: 3.698713221640033\n",
      "MAE on test set: 3.3494474448901776\n",
      "___________________________\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 3.7194\n",
      "MAE:  3.3456\n",
      "RMSE on test set: 3.7194109611329065\n",
      "MAE on test set: 3.3456139855661795\n",
      "___________________________\n"
     ]
    }
   ],
   "source": [
    "# Collaborative filtering\n",
    "\n",
    "# Define the features\n",
    "collaborative_features = ['User-ID', 'ISBN', 'Book-Rating']\n",
    "\n",
    "collab_train=Dataset.load_from_df(trainset[collaborative_features], reader)\n",
    "collab_test=Dataset.load_from_df(testset[collaborative_features], reader)\n",
    "\n",
    "baseline = BaselineOnly()\n",
    "KNN = KNNBasic(sim_options={'user_based': True})\n",
    "\n",
    "# cross_validate(baseline, collab_train, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "collab_trainset=collab_train.build_full_trainset()\n",
    "collab_testset=collab_test.build_full_trainset().build_testset()\n",
    "\n",
    "\n",
    "baseline.fit(collab_trainset)\n",
    "predictions = baseline.test(collab_testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "print(f'RMSE on test set: {rmse}')\n",
    "print(f'MAE on test set: {mae}')\n",
    "print('___________________________')\n",
    "\n",
    "\n",
    "KNN.fit(collab_trainset)\n",
    "predictions = KNN.test(collab_testset)\n",
    "rmse = accuracy.rmse(predictions)\n",
    "mae = accuracy.mae(predictions)\n",
    "print(f'RMSE on test set: {rmse}')\n",
    "print(f'MAE on test set: {mae}')\n",
    "print('___________________________')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fd0cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c01e719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Content-based filtering\n",
    "\n",
    "\n",
    "content_based_features = ['Book-Title', \n",
    "                          'Book-Author', \n",
    "                          'Year-Of-Publication',\n",
    "#                          'Publisher',\n",
    "                          'AgeGroup',\n",
    "#                           'State',\n",
    "                          'City'\n",
    "                         ]\n",
    "\n",
    "trainset_X = trainset[content_based_features]\n",
    "trainset_y = trainset['Book-Rating']\n",
    "testset_X = testset[content_based_features]\n",
    "testset_y = testset['Book-Rating']\n",
    "\n",
    "# Encoding\n",
    "\n",
    "\n",
    "categorical_attributes = list(trainset_X.select_dtypes(include=['object']).columns)\n",
    "numerical_attributes = list(trainset_X.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "                         ('std_scaler', StandardScaler()),\n",
    "                        ])\n",
    "full_pipeline = ColumnTransformer([('num', num_pipeline, numerical_attributes),\n",
    "                                   ('cat', OneHotEncoder(), categorical_attributes),\n",
    "                                  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e28de2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((411152, 188526), (411152, 5))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = full_pipeline.fit_transform(trainset_X)\n",
    "train_labels = trainset_y\n",
    "train.shape ,trainset_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_train_rmse 2.7520909647897973\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train, train_labels)\n",
    "\n",
    "area_predictions = lin_reg.predict(train)\n",
    "lin_mse = mean_squared_error(train_labels, area_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print('linear_train_rmse', lin_rmse) \n",
    "\n",
    "\n",
    "scores = cross_val_score(lin_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "def explain_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    " \n",
    "explain_scores(lin_rmse_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "dump(lin_reg, 'lin_reg.joblib') \n",
    "# model=load('lin_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_reg.fit(train, train_labels)\n",
    "\n",
    "area_predictions = knn_reg.predict(train)\n",
    "knn_mse = mean_squared_error(train_labels, area_predictions)\n",
    "knn_rmse = np.sqrt(knn_mse)\n",
    "print('knn_train_rmse', knn_rmse) #overfiiting\n",
    "\n",
    "scores = cross_val_score(knn_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "knn_rmse_scores = np.sqrt(-scores)\n",
    "explain_scores(knn_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7092b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "dump(knn_reg, 'knn_reg.joblib') \n",
    "# model=load('knn_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0baa9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "svm_reg = SVR(kernel='linear')\n",
    "svm_reg.fit(train, train_labels)\n",
    "\n",
    "area_predictions = svm_reg.predict(train)\n",
    "svm_mse = mean_squared_error(train_labels, area_predictions)\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "print('svm_train_rmse', svm_rmse) #svm is generalizing well to crossvalidation set\n",
    "\n",
    "scores = cross_val_score(svm_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "svm_rmse_scores = np.sqrt(-scores)\n",
    "explain_scores(svm_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "dump(svm_reg, 'svm_reg.joblib') \n",
    "# model=load('svm_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fdf2652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree_train_rmse 0.0031622776601683794\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(train, train_labels)\n",
    "\n",
    "area_predictions = tree_reg.predict(train)\n",
    "tree_mse = mean_squared_error(train_labels, area_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print('tree_train_rmse', tree_rmse)\n",
    "\n",
    "scores = cross_val_score(tree_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "explain_scores(tree_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "dump(tree_reg, 'tree_reg.joblib') \n",
    "# model=load('tree_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e83eb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 16.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_jobs=-1, verbose=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, verbose=True)\n",
    "rf_reg.fit(train, train_labels)\n",
    "\n",
    "area_predictions = rf_reg.predict(train)\n",
    "rf_mse = mean_squared_error(train_labels, area_predictions)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "print('rf_train_rmse', rf_rmse) \n",
    "\n",
    "scores = cross_val_score(rf_reg, train, train_labels, scoring='neg_mean_squared_error', cv=10)\n",
    "rf_rmse_scores = np.sqrt(-scores)\n",
    "explain_scores(rf_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82094f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "dump(rf_reg, 'rf_reg.joblib') \n",
    "# model=load('rf_reg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67484a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb15cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec7b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "121a0ff5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but DecisionTreeRegressor is expecting 36959 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2961575/2082494550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0muser_id_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m278838\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0misbn_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0440400988'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprediction_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhybrid_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_id_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misbn_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Hybrid Prediction for User {user_id_example}, ISBN {isbn_example}: {prediction_example}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2961575/2082494550.py\u001b[0m in \u001b[0;36mhybrid_predict\u001b[0;34m(user_id, isbn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Content-based filtering prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcontent_based_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_based_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misbn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Combine predictions (you can customize the weighting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[1;32m    408\u001b[0m                                     reset=False)\n\u001b[1;32m    409\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1 features, but DecisionTreeRegressor is expecting 36959 features as input."
     ]
    }
   ],
   "source": [
    "# Combine collaborative and content-based predictions\n",
    "def hybrid_predict(user_id, isbn):\n",
    "    # Collaborative filtering prediction\n",
    "    collaborative_prediction = --collaborative_algo--.predict(user_id, isbn).est\n",
    "\n",
    "    # Content-based filtering prediction\n",
    "    content_based_prediction = --content_based_pipeline--.predict([[----------]])\n",
    "\n",
    "    # Combine predictions (you can customize the weighting)\n",
    "    hybrid_prediction = 0.7 * collaborative_prediction + 0.3 * content_based_prediction\n",
    "\n",
    "    return hybrid_prediction\n",
    "\n",
    "\n",
    "user_id_example = 278838\n",
    "isbn_example = '0440400988'\n",
    "prediction_example = hybrid_predict(user_id_example, isbn_example)\n",
    "print(f'Hybrid Prediction for User {user_id_example}, ISBN {isbn_example}: {prediction_example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e076dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborative_prediction = collaborative_algo.predict(user_id_example, isbn_example).est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1e3683f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.464030128128015"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborative_prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
